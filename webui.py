#!/usr/bin/env python3
"""
Kyutai STT & TTS - Complete WebUI
All repository features in one simple interface
"""

import os
import sys
import shutil
from pathlib import Path

# Setup portable environment
project_dir = Path(__file__).parent.absolute()
os.environ['HF_HOME'] = str(project_dir / "cache")
os.environ['HF_HUB_CACHE'] = str(project_dir / "cache")
os.environ['TRANSFORMERS_CACHE'] = str(project_dir / "cache")
os.environ['TEMP'] = str(project_dir / "temp")
os.environ['TMP'] = str(project_dir / "temp")

# Create directories
(project_dir / "cache").mkdir(exist_ok=True)
(project_dir / "temp").mkdir(exist_ok=True)

print(f"Using cache: {project_dir / 'cache'}")
print(f"Using temp: {project_dir / 'temp'}")

import gradio as gr
import torch
import numpy as np
import tempfile
import subprocess
import socket
from typing import Tuple, List, Optional

# Import Kyutai libraries
try:
    import sphn
    import sounddevice as sd
    from moshi.models.loaders import CheckpointInfo
    from moshi.models.tts import DEFAULT_DSM_TTS_REPO, DEFAULT_DSM_TTS_VOICE_REPO, TTSModel
    import moshi.models
    HAS_MOSHI = True
    print("‚úÖ Moshi loaded successfully")
except ImportError as e:
    pass
    HAS_MOSHI = False
    print(f"‚ùå Moshi not available: {e}")

# Check for audio libraries
try:
    import soundfile as sf
    HAS_SOUNDFILE = True
    print("‚úÖ soundfile available")
except ImportError:
    HAS_SOUNDFILE = False
    print("‚ö†Ô∏è soundfile not available, using alternative")

try:
    from scipy import signal
    HAS_SCIPY = True
    print("‚úÖ scipy available")
except ImportError:
    HAS_SCIPY = False
    print("‚ö†Ô∏è scipy not available, using basic features")

try:
    import mlx.core as mx
    import mlx.nn as nn
    from moshi_mlx import models, utils
    HAS_MLX = True
    print("‚úÖ MLX loaded successfully")
except ImportError:
    pass
    HAS_MLX = False
    print("‚ÑπÔ∏è MLX not available (Apple Silicon only)")

# Global model cache
model_cache = {}

def get_available_lora_adapters():
    """Get actually available LoRA adapters from filesystem"""
    lora_dir = project_dir / "lora_adapters"
    available_loras = ["None"]
    
    if lora_dir.exists():
        for lora_path in lora_dir.iterdir():
            if lora_path.is_dir() and (lora_path / "adapter_config.json").exists():
                available_loras.append(lora_path.name)
    
    return available_loras

def analyze_official_voice_structure():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞"""
    try:
        # –ü—É—Ç—å –∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º—É –≥–æ–ª–æ—Å—É
        official_voice_path = project_dir / "cache" / "models--kyutai--tts-voices" / "snapshots"
        
        # –ù–∞–π—Ç–∏ –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –≥–æ–ª–æ—Å
        for snapshot_dir in official_voice_path.glob("*"):
            if snapshot_dir.is_dir():
                expresso_dir = snapshot_dir / "expresso"
                if expresso_dir.exists():
                    safetensors_files = list(expresso_dir.glob("*.safetensors"))
                    if safetensors_files:
                        official_voice = safetensors_files[0]
                        print(f"üîç Analyzing official voice: {official_voice.name}")
                        print(f"üìè File size: {official_voice.stat().st_size} bytes ({official_voice.stat().st_size/1024:.1f} KB)")
                        
                        from safetensors.torch import load_file
                        voice_data = load_file(str(official_voice))
                        
                        print(f"üîë Keys in official voice: {list(voice_data.keys())}")
                        for key, tensor in voice_data.items():
                            print(f"  {key}: shape={tensor.shape}, dtype={tensor.dtype}, numel={tensor.numel()}")
                        
                        return voice_data
        
        print("‚ùå No official voices found")
        return None
        
    except Exception as e:
        print(f"‚ùå Error analyzing official voice: {e}")
        return None

def get_local_voices():
    """Get available voices from local cache and custom voices"""
    all_voices = []
    
    # Check for local voice files from cache
    cache_dir = project_dir / "cache" / "voices"
    if cache_dir.exists():
        for voice_file in cache_dir.rglob("*.safetensors"):
            # Convert path to voice name format
            relative_path = voice_file.relative_to(cache_dir)
            voice_name = str(relative_path).replace(".safetensors", "").replace("\\", "/")
            all_voices.append(f"üìª {voice_name}") # Add radio emoji for cached voices
    
    # Check custom voices directory (trained voices)
    custom_voices_dir = project_dir / "custom_voices"
    if custom_voices_dir.exists():
        for voice_file in custom_voices_dir.rglob("*.safetensors"):
            relative_path = voice_file.relative_to(custom_voices_dir)
            path_str = str(relative_path).replace('.safetensors', '').replace('\\', '/')
            
            # Try to load metadata for better display name
            metadata_file = voice_file.parent / f"{voice_file.stem}_metadata.json"
            display_name = path_str
            
            if metadata_file.exists():
                try:
                    import json
                    with open(metadata_file, 'r') as f:
                        metadata = json.load(f)
                    emotion = metadata.get('emotion', 'unknown')
                    speaker = metadata.get('speaker', 'unknown')
                    display_name = f"{metadata.get('name', path_str)} ({emotion})"
                except:
                    pass
            
            voice_name = f"custom/{path_str}"
            all_voices.append(f"üé§ {display_name}")
            # Store mapping for actual usage
            if not hasattr(get_local_voices, 'voice_mapping'):
                get_local_voices.voice_mapping = {}
            get_local_voices.voice_mapping[f"üé§ {display_name}"] = voice_name
            
            # Also store direct file path mapping for preview generation
            if not hasattr(get_local_voices, 'file_mapping'):
                get_local_voices.file_mapping = {}
            get_local_voices.file_mapping[f"üé§ {display_name}"] = str(voice_file)
    
    # Combine all voices
    final_voices = all_voices
    
    return final_voices if final_voices else []
def get_available_lora_adapters():
    """Get actually available LoRA adapters from filesystem"""
    lora_dir = project_dir / "lora_adapters"
    available_loras = ["None"]
    
    if lora_dir.exists():
        for lora_path in lora_dir.iterdir():
            if lora_path.is_dir() and (lora_path / "adapter_config.json").exists():
                available_loras.append(lora_path.name)
    
    return available_loras

def get_available_models():
    """Get all available models"""
    stt_models = [
        "kyutai/stt-2.6b-en",
        "kyutai/stt-1b-en_fr"
    ]
    
    tts_models = [
        DEFAULT_DSM_TTS_REPO if HAS_MOSHI else "kyutai/dsm-tts-1b-en"
    ]
    
    voices = get_local_voices()
    
    return stt_models, tts_models, voices

def load_stt_model(model_name: str, device: str = "cuda"):
    """Load STT model"""
    cache_key = f"stt_{model_name}_{device}"
    
    if cache_key not in model_cache:
        if not HAS_MOSHI:
            raise ImportError("Moshi not installed")
        
        print(f"Loading STT model: {model_name}")
        checkpoint_info = CheckpointInfo.from_hf_repo(model_name)
        mimi = checkpoint_info.get_mimi(device=device)
        tokenizer = checkpoint_info.get_text_tokenizer()
        lm = checkpoint_info.get_moshi(device=device, dtype=torch.bfloat16)
        lm_gen = moshi.models.LMGen(lm, temp=0, temp_text=0.0)
        
        model_cache[cache_key] = {
            'mimi': mimi,
            'tokenizer': tokenizer,
            'lm_gen': lm_gen,
            'checkpoint_info': checkpoint_info
        }
        print(f"‚úÖ STT model loaded: {model_name}")
    
    return model_cache[cache_key]

def load_tts_model(model_name: str, device: str = "cuda"):
    """Load TTS model"""
    cache_key = f"tts_{model_name}_{device}"
    
    if cache_key not in model_cache:
        if not HAS_MOSHI:
            raise ImportError("Moshi not installed")
        
        print(f"Loading TTS model: {model_name}")
        checkpoint_info = CheckpointInfo.from_hf_repo(model_name)
        tts_model = TTSModel.from_checkpoint_info(
            checkpoint_info, n_q=32, temp=0.6, device=device
        )
        
        model_cache[cache_key] = tts_model
        print(f"‚úÖ TTS model loaded: {model_name}")
    
    return model_cache[cache_key]

def transcribe_audio(audio_file, model_name: str, device: str = "cuda") -> str:
    """Transcribe audio file"""
    if not audio_file:
        return "No audio file provided"
    
    if not HAS_MOSHI:
        return "‚ùå Moshi not installed. Please run setup."
    
    try:
        # Load model
        model_data = load_stt_model(model_name, device)
        mimi = model_data['mimi']
        tokenizer = model_data['tokenizer']
        lm_gen = model_data['lm_gen']
        checkpoint_info = model_data['checkpoint_info']
        
        # Load and process audio
        audio, sample_rate = sphn.read(audio_file)
        audio = torch.from_numpy(audio).to(device).mean(axis=0, keepdim=True)
        
        # Resample if needed
        if sample_rate != mimi.sample_rate:
            import julius
            audio = julius.resample_frac(audio, sample_rate, mimi.sample_rate)
        
        # Pad to frame size
        if audio.shape[-1] % mimi.frame_size != 0:
            to_pad = mimi.frame_size - audio.shape[-1] % mimi.frame_size
            audio = torch.nn.functional.pad(audio, (0, to_pad))
        
        # Transcribe
        text_tokens_accum = []
        audio_silence_prefix_seconds = checkpoint_info.stt_config.get("audio_silence_prefix_seconds", 1.0)
        audio_delay_seconds = checkpoint_info.stt_config.get("audio_delay_seconds", 5.0)
        padding_token_id = checkpoint_info.raw_config.get("text_padding_token_id", 3)
        
        # Process with silence padding
        n_prefix_chunks = int(audio_silence_prefix_seconds * mimi.frame_rate)
        n_suffix_chunks = int(audio_delay_seconds * mimi.frame_rate)
        silence_chunk = torch.zeros((1, 1, mimi.frame_size), dtype=torch.float32, device=device)
        
        with mimi.streaming(1), lm_gen.streaming(1):
            # Prefix silence
            for _ in range(n_prefix_chunks):
                audio_tokens = mimi.encode(silence_chunk)
                text_tokens = lm_gen.step(audio_tokens)
                if text_tokens is not None:
                    text_tokens_accum.append(text_tokens)
            
            # Process audio
            for i in range(0, audio.shape[-1], mimi.frame_size):
                chunk = audio[:, None, i:i + mimi.frame_size]
                if chunk.shape[-1] < mimi.frame_size:
                    chunk = torch.nn.functional.pad(chunk, (0, mimi.frame_size - chunk.shape[-1]))
                
                audio_tokens = mimi.encode(chunk)
                text_tokens = lm_gen.step(audio_tokens)
                if text_tokens is not None:
                    text_tokens_accum.append(text_tokens)
            
            # Suffix silence
            for _ in range(n_suffix_chunks):
                audio_tokens = mimi.encode(silence_chunk)
                text_tokens = lm_gen.step(audio_tokens)
                if text_tokens is not None:
                    text_tokens_accum.append(text_tokens)
        
        # Decode text
        if text_tokens_accum:
            utterance_tokens = torch.concat(text_tokens_accum, dim=-1)
            text_tokens = utterance_tokens.cpu().view(-1)
            text = tokenizer.decode(text_tokens[text_tokens > padding_token_id].numpy().tolist())
            return text.strip()
        else:
            return "No speech detected"
            
    except Exception as e:
        return f"‚ùå Error: {str(e)}"

def synthesize_speech(text: str, voice: str, model_name: str, device: str = "cuda") -> Tuple[int, np.ndarray]:
    """Synthesize speech from text"""
    if not text.strip():
        return 24000, np.array([0.0])  # Return small non-empty array
    
    if not HAS_MOSHI:
        print("‚ùå Moshi not available")
        return 24000, np.array([0.0])
    
    # Force CPU if CUDA not available
    if device == "cuda" and not torch.cuda.is_available():
        print("‚ö†Ô∏è CUDA not available, using CPU")
        device = "cpu"
    
    try:
        # Check disk space
        import shutil
        free_space = shutil.disk_usage('.').free / (1024**3)
        if free_space < 5:
            print(f"‚ö†Ô∏è Low disk space: {free_space:.1f}GB")
        
        print(f"Loading TTS model on {device}...")
        
        # Load model
        tts_model = load_tts_model(model_name, device)
        
        print(f"Preparing text: {text[:50]}...")
        
        # Prepare text
        entries = tts_model.prepare_script([text.strip()], padding_between=1)
        
        # Convert display name back to actual voice path and resolve local path
        voice_path = None
        actual_voice_name = voice

        if voice.startswith("üé§ "):
            # –î–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
            print(f"DEBUG: Processing custom voice: {voice}")
            
            # –ù–∞–π—Ç–∏ —Ñ–∞–π–ª –Ω–∞–ø—Ä—è–º—É—é –≤ custom_voices
            custom_voices_dir = project_dir / "custom_voices"
            
            # –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º safetensors —Ñ–∞–π–ª–∞–º –≤ custom_voices
            found_voice = None
            for voice_file in custom_voices_dir.rglob("*.safetensors"):
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
                metadata_file = voice_file.parent / f"{voice_file.stem}_metadata.json"
                if metadata_file.exists():
                    try:
                        import json
                        with open(metadata_file, 'r') as f:
                            metadata = json.load(f)
                        
                        # –°–æ–∑–¥–∞—Ç—å display name –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                        display_name = f"{metadata.get('name', voice_file.stem)} ({metadata.get('emotion', 'unknown')})"
                        if voice == f"üé§ {display_name}":
                            found_voice = voice_file
                            break
                    except:
                        pass
                
                # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                if voice_file.stem in voice or voice_file.name in voice:
                    found_voice = voice_file
                    break
            
            if found_voice:
                voice_path = found_voice
                print(f"DEBUG: ‚úÖ Found custom voice: {voice_path}")
            else:
                print(f"DEBUG: ‚ùå Custom voice not found for: {voice}")
                voice_path = None
        elif voice.startswith("üì¶ ") or voice.startswith("üìª "):
            actual_voice_name = voice[2:]  # Remove emoji
            # Cached voices are stored directly under cache/voices/expresso/voice.safetensors or cache/voices/cml-tts/...
            voice_path = project_dir / "cache" / "voices" / f"{actual_voice_name}.safetensors"
            if not voice_path.exists():
                print(f"DEBUG: ‚ùå Cached voice not found at expected path: {voice_path}")
                voice_path = None # Reset if not found
            else:
                print(f"DEBUG: ‚úÖ Found cached voice locally: {voice_path}")
        
        # If voice_path is still None, it means it's not a recognized local path or it didn't exist.
        # In this case, try to resolve it via tts_model.get_voice_path (which might download).
        if voice_path is None:
            print(f"DEBUG: Attempting to resolve voice via tts_model.get_voice_path (might download): {actual_voice_name}")
            try:
                voice_path = tts_model.get_voice_path(actual_voice_name)
                print(f"DEBUG: ‚úÖ Resolved voice via tts_model.get_voice_path: {voice_path}")
            except Exception as e:
                print(f"DEBUG: ‚ùå Failed to resolve voice via tts_model.get_voice_path: {actual_voice_name}, error: {e}")
                voice_path = None # Ensure it's None if resolution fails

        if voice_path:
            print(f"DEBUG: Final voice_path resolved to: {voice_path}")
            try:
                # For local .safetensors files, create proper condition attributes
                if str(voice_path).endswith('.safetensors'):
                    print(f"DEBUG: Loading local safetensors voice embedding from: {voice_path}")
                    from safetensors.torch import load_file
                    voice_data = load_file(str(voice_path))
                    print(f"DEBUG: Local safetensors voice data keys: {voice_data.keys()}")
                    
                    if 'speaker_wavs' in voice_data:
                        speaker_wavs = voice_data['speaker_wavs']
                        print(f"DEBUG: Loaded speaker_wavs shape: {speaker_wavs.shape}")
                        
                        try:
                            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥ - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–æ–º
                            print(f"DEBUG: Trying to use existing make_condition_attributes method")
                            
                            # –°–æ–∑–¥–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ø–∏—é —Ñ–∞–π–ª–∞ –≤ cache/voices –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                            import tempfile
                            import shutil
                            
                            with tempfile.NamedTemporaryFile(suffix='.safetensors', delete=False) as temp_file:
                                shutil.copy2(voice_path, temp_file.name)
                                temp_path = temp_file.name
                            
                            try:
                                condition_attributes = tts_model.make_condition_attributes([temp_path], cfg_coef=2.0)
                                print(f"DEBUG: ‚úÖ Successfully created condition attributes using temp file")
                            finally:
                                # –û—á–∏—Å—Ç–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                                import os
                                try:
                                    os.unlink(temp_path)
                                except:
                                    pass
                            
                        except Exception as e:
                            print(f"DEBUG: ‚ùå Error with temp file approach: {e}")
                            print("DEBUG: Falling back to no voice conditioning")
                            condition_attributes = None
                    else:
                        print("DEBUG: ‚ùå No 'speaker_wavs' found in local safetensors file.")
                        condition_attributes = None
                else:
                    # For non-.safetensors paths (e.g., direct audio files or paths resolved by get_voice_path that are not .safetensors)
                    print(f"DEBUG: Attempting to create standard condition attributes from non-safetensors path: {voice_path}")
                    condition_attributes = tts_model.make_condition_attributes([voice_path], cfg_coef=2.0)
                    print(f"DEBUG: Created standard condition attributes")
            except Exception as e:
                print(f"DEBUG: ‚ùå Error loading voice or creating condition attributes: {e}")
                print("DEBUG: Falling back to no voice conditioning")
                condition_attributes = None
        else:
            print("DEBUG: No voice path resolved, setting condition_attributes to None")
            condition_attributes = None
        
        print(f"DEBUG: Final condition_attributes before generate: {condition_attributes}")
        print("Generating audio...")
        
        # Generate audio
        if condition_attributes is None:
            result = tts_model.generate([entries], []) # Pass empty list if no conditioning
        else:
            result = tts_model.generate([entries], [condition_attributes])
        print(f"Generated {len(result.frames)} frames")
        
        # Decode audio
        with tts_model.mimi.streaming(1), torch.no_grad():
            pcms = []
            for i, frame in enumerate(result.frames[tts_model.delay_steps:]):
                if i % 10 == 0:  # Progress indicator
                    print(f"Decoding frame {i}/{len(result.frames)-tts_model.delay_steps}")
                
                pcm = tts_model.mimi.decode(frame[:, 1:, :]).cpu().numpy()
                pcm_clipped = np.clip(pcm[0, 0], -1, 1)
                
                # Check for valid audio
                if pcm_clipped.size > 0 and not np.all(pcm_clipped == 0):
                    pcms.append(pcm_clipped)
            
            if pcms and len(pcms) > 0:
                audio = np.concatenate(pcms, axis=-1)
                print(f"‚úÖ Generated audio: {len(audio)} samples, {len(audio)/tts_model.mimi.sample_rate:.2f}s")
                
                # Ensure audio is not empty and has valid range
                if len(audio) > 0 and np.max(np.abs(audio)) > 0:
                    return tts_model.mimi.sample_rate, audio.astype(np.float32)
                else:
                    print("‚ö†Ô∏è Generated audio is silent")
                    # Return a short beep instead of empty array
                    sample_rate = tts_model.mimi.sample_rate
                    duration = 0.1  # 100ms beep
                    t = np.linspace(0, duration, int(sample_rate * duration))
                    beep = 0.1 * np.sin(2 * np.pi * 440 * t)  # 440Hz beep
                    return sample_rate, beep.astype(np.float32)
            else:
                print("‚ö†Ô∏è No audio frames generated")
                # Return a short beep
                sample_rate = 24000
                duration = 0.1
                t = np.linspace(0, duration, int(sample_rate * duration))
                beep = 0.1 * np.sin(2 * np.pi * 440 * t)
                return sample_rate, beep.astype(np.float32)
                
    except Exception as e:
        print(f"‚ùå TTS Error: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # Return error beep instead of empty array
        sample_rate = 24000
        duration = 0.2
        t = np.linspace(0, duration, int(sample_rate * duration))
        error_beep = 0.1 * np.sin(2 * np.pi * 220 * t)  # Lower frequency for error
        return sample_rate, error_beep.astype(np.float32)

def run_script(script_name: str, *args) -> str:
    """Run repository script"""
    script_path = project_dir / "scripts" / script_name
    if not script_path.exists():
        return f"‚ùå Script not found: {script_name}"
    
    try:
        cmd = [sys.executable, str(script_path)] + list(args)
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        
        if result.returncode == 0:
            return result.stdout
        else:
            return f"‚ùå Error: {result.stderr}"
    except subprocess.TimeoutExpired:
        return "‚ùå Script timed out"
    except Exception as e:
        return f"‚ùå Error: {str(e)}"

def batch_transcribe(files) -> str:
    """Batch transcribe multiple files"""
    if not files:
        return "No files provided"
    
    results = []
    for i, file in enumerate(files):
        results.append(f"File {i+1}: {file.name}")
        transcription = transcribe_audio(file.name, "kyutai/stt-2.6b-en", "cuda")
        results.append(f"Result: {transcription}")
        results.append("-" * 50)
    
    return "\n".join(results)

def find_free_port(start_port=7860, max_attempts=10):
    """Find available port"""
    for port in range(start_port, start_port + max_attempts):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('localhost', port))
                return port
        except OSError:
            continue
    return None
def create_interface():
    """Create the main interface"""
    
    stt_models, tts_models, voices = get_available_models()
    
    # Auto-detect best device
    if torch.cuda.is_available():
        default_device = "cuda"
        device_status = f"üöÄ GPU: {torch.cuda.get_device_name()}"
    else:
        default_device = "cpu"
        device_status = "üíª CPU Mode"
    
    with gr.Blocks(title="Kyutai STT & TTS", theme=gr.themes.Soft()) as demo:
        
        gr.Markdown(f"""
        # üéôÔ∏è Kyutai STT & TTS - Complete Interface
        
        Full-featured web interface for Kyutai Speech-to-Text and Text-to-Speech models.
        All repository features available in one simple interface.
        
        **Status**: {device_status} | **Moshi**: {'‚úÖ' if HAS_MOSHI else '‚ùå'} | **MLX**: {'‚úÖ' if HAS_MLX else '‚ùå'}
        """)
        
        # STT Tab
        with gr.Tab("üéôÔ∏è Speech-to-Text"):
            with gr.Row():
                with gr.Column():
                    stt_audio = gr.Audio(label="Audio Input", sources=["upload", "microphone"], type="filepath")
                    stt_model = gr.Dropdown(choices=stt_models, value=stt_models[0], label="Base Model")
                    stt_lora = gr.Dropdown(choices=get_available_lora_adapters(), value="None", label="LoRA Adapter")
                    stt_device = gr.Dropdown(choices=["cuda", "cpu"], value=default_device, label="Device")
                    stt_btn = gr.Button("üéØ Transcribe", variant="primary")
                
                with gr.Column():
                    stt_output = gr.Textbox(label="Transcription", lines=15, show_copy_button=True)
            
            def transcribe_with_lora(audio, model, lora, device):
                # TODO: –†–µ–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è LoRA
                if lora != "None":
                    result = transcribe_audio(audio, model, device)
                    return f"[Using {lora}] {result}"
                else:
                    return transcribe_audio(audio, model, device)
            
            stt_btn.click(transcribe_with_lora, inputs=[stt_audio, stt_model, stt_lora, stt_device], outputs=stt_output)
        
        # TTS Tab
        with gr.Tab("üó£Ô∏è Text-to-Speech"):
            with gr.Row():
                with gr.Column():
                    tts_text = gr.Textbox(label="Text to Synthesize", lines=5, placeholder="Enter text...")
                    
                    with gr.Row():
                        tts_voice = gr.Dropdown(choices=voices, value=voices[0], label="Voice", scale=4)
                        refresh_voices_btn = gr.Button("üîÑ", scale=1, size="sm")
                    
                    tts_model = gr.Dropdown(choices=tts_models, value=tts_models[0], label="Base Model")
                    tts_lora = gr.Dropdown(choices=get_available_lora_adapters(), value="None", label="LoRA Adapter")
                    tts_device = gr.Dropdown(choices=["cuda", "cpu"], value=default_device, label="Device")
                    tts_btn = gr.Button("üéµ Synthesize", variant="primary")
                
                with gr.Column():
                    tts_output = gr.Audio(label="Generated Audio", type="numpy")
                    
                    # Direct download button - downloads exactly what you hear
                    download_btn = gr.DownloadButton(
                        label="üíæ Download Audio",
                        variant="secondary"
                    )
                    
                    def prepare_download(audio_data):
                        """Prepare audio file for download - exactly what you hear"""
                        if audio_data is None:
                            return None
                        
                        try:
                            sample_rate, audio_array = audio_data
                            
                            # Create temporary file with timestamp
                            import datetime
                            import tempfile
                            import wave
                            
                            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                            temp_file = tempfile.NamedTemporaryFile(
                                delete=False, 
                                suffix=f"_tts_{timestamp}.wav"
                            )
                            
                            # Convert float32 to int16 for WAV format
                            audio_int16 = (audio_array * 32767).astype(np.int16)
                            
                            # Save using wave module (built-in Python)
                            with wave.open(temp_file.name, 'wb') as wav_file:
                                wav_file.setnchannels(1)  # Mono
                                wav_file.setsampwidth(2)  # 16-bit
                                wav_file.setframerate(sample_rate)
                                wav_file.writeframes(audio_int16.tobytes())
                            
                            return temp_file.name
                            
                        except Exception as e:
                            print(f"Error preparing download: {e}")
                            return None
                    
                    # Update download button when audio is generated
                    def update_download_button(audio_data):
                        if audio_data is not None:
                            file_path = prepare_download(audio_data)
                            if file_path:
                                return file_path
                        return None
                    
                    tts_output.change(
                        fn=update_download_button,
                        inputs=tts_output,
                        outputs=download_btn
                    )
            
            def synthesize_with_lora(text, voice, model, lora, device):
                # TODO: –†–µ–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è LoRA
                if lora != "None":
                    print(f"Using LoRA: {lora}")
                
                return synthesize_speech(text, voice, model, device)
            
            def refresh_voices():
                """Refresh the voice list to include newly trained voices"""
                updated_voices = get_local_voices()
                return gr.Dropdown(choices=updated_voices, value=updated_voices[0] if updated_voices else None)
            
            tts_btn.click(
                synthesize_with_lora, 
                inputs=[tts_text, tts_voice, tts_model, tts_lora, tts_device], 
                outputs=tts_output
            )
            
            refresh_voices_btn.click(
                refresh_voices,
                outputs=tts_voice
            )
        
        # Batch Tab
        with gr.Tab("üì¶ Batch Processing"):
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### Batch STT")
                    batch_files = gr.File(label="Audio Files", file_count="multiple", file_types=["audio"])
                    batch_btn = gr.Button("Process All")
                    batch_output = gr.Textbox(label="Results", lines=20)
                
                batch_btn.click(batch_transcribe, inputs=batch_files, outputs=batch_output)
        
        # Scripts Tab
        with gr.Tab("‚öôÔ∏è Scripts"):
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### Repository Scripts")
                    script_file = gr.File(label="Input File", file_types=["audio", "txt"])
                    script_name = gr.Dropdown(
                        choices=[
                            "stt_from_file_pytorch.py",
                            "tts_pytorch.py",
                            "tts_pytorch_streaming.py"
                        ],
                        label="Script"
                    )
                    script_btn = gr.Button("Run Script")
                    script_output = gr.Textbox(label="Output", lines=15)
                
                def run_selected_script(file, script):
                    if file and script:
                        return run_script(script, file.name)
                    return "Please select file and script"
                
                script_btn.click(run_selected_script, inputs=[script_file, script_name], outputs=script_output)
        
        
        # Training Tab - Simple and Clear
        with gr.Tab("üéì Training(WORK IN PROGRESS)"):
            gr.Markdown("## Model Training - 4 Training Modes")
            
            with gr.Tabs():
                # Mode 1: Fine-tune Model
                with gr.TabItem("1Ô∏è‚É£ Fine-tune Model"):
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown("### Configuration")
                            ft_base_model = gr.Dropdown(
                                choices=["kyutai/stt-2.6b-en", "kyutai/stt-1b-en_fr"],
                                label="Base Model",
                                value="kyutai/stt-1b-en_fr"
                            )
                            ft_dataset_path = gr.Textbox(label="Dataset Path", placeholder="/path/to/dataset")
                            ft_output_dir = gr.Textbox(label="Output Directory", value="./fine_tuned_model")
                            
                            with gr.Row():
                                ft_epochs = gr.Slider(1, 100, value=10, label="Epochs")
                                ft_batch_size = gr.Slider(1, 32, value=4, label="Batch Size")
                            
                            with gr.Row():
                                ft_learning_rate = gr.Number(value=3e-5, label="Learning Rate")
                                ft_warmup_steps = gr.Number(value=1000, label="Warmup Steps")
                            
                            ft_start_btn = gr.Button("üöÄ Start Fine-tuning", variant="primary")
                            ft_stop_btn = gr.Button("‚èπÔ∏è Stop Training", variant="secondary")
                        
                        with gr.Column():
                            gr.Markdown("### Training Status")
                            ft_status = gr.Textbox(label="Status", value="Ready", interactive=False)
                            ft_progress = gr.Slider(0, 100, value=0, label="Progress (%)", interactive=False)
                            ft_loss_plot = gr.Plot(label="Training Loss")
                            ft_logs = gr.Textbox(label="Training Logs", lines=8, interactive=False)
                
                # Mode 2: Train from Scratch
                with gr.TabItem("2Ô∏è‚É£ Train from Scratch"):
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown("### Configuration")
                            scratch_model_type = gr.Dropdown(
                                choices=["STT", "TTS"],
                                label="Model Type",
                                value="STT"
                            )
                            scratch_dataset_path = gr.Textbox(label="Dataset Path", placeholder="/path/to/large_dataset")
                            scratch_output_dir = gr.Textbox(label="Output Directory", value="./new_model")
                            
                            with gr.Row():
                                scratch_vocab_size = gr.Number(value=8000, label="Vocabulary Size")
                                scratch_model_size = gr.Dropdown(
                                    choices=["small", "medium", "large"],
                                    label="Model Size",
                                    value="medium"
                                )
                            
                            with gr.Row():
                                scratch_epochs = gr.Slider(1, 200, value=50, label="Epochs")
                                scratch_batch_size = gr.Slider(1, 16, value=2, label="Batch Size")
                            
                            scratch_start_btn = gr.Button("üöÄ Start Training", variant="primary")
                            scratch_stop_btn = gr.Button("‚èπÔ∏è Stop Training", variant="secondary")
                        
                        with gr.Column():
                            gr.Markdown("### Training Status")
                            scratch_status = gr.Textbox(label="Status", value="Ready", interactive=False)
                            scratch_progress = gr.Slider(0, 100, value=0, label="Progress (%)", interactive=False)
                            scratch_loss_plot = gr.Plot(label="Training Loss")
                            scratch_logs = gr.Textbox(label="Training Logs", lines=8, interactive=False)
                
                # Mode 3: LoRA Training
                with gr.TabItem("3Ô∏è‚É£ LoRA Training"):
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown("### Configuration")
                            lora_base_model = gr.Dropdown(
                                choices=["kyutai/stt-2.6b-en", "kyutai/stt-1b-en_fr"],
                                label="Base Model",
                                value="kyutai/stt-1b-en_fr"
                            )
                            lora_dataset_path = gr.Textbox(label="Dataset Path", placeholder="/path/to/dataset")
                            lora_output_dir = gr.Textbox(label="Output Directory", value="./lora_adapters")
                            
                            with gr.Row():
                                lora_rank = gr.Slider(4, 128, value=32, label="LoRA Rank")
                                lora_alpha = gr.Slider(8, 256, value=64, label="LoRA Alpha")
                            
                            with gr.Row():
                                lora_dropout = gr.Slider(0.0, 0.5, value=0.1, label="Dropout")
                                lora_epochs = gr.Slider(1, 50, value=10, label="Epochs")
                            
                            lora_target_modules = gr.CheckboxGroup(
                                choices=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
                                value=["q_proj", "v_proj", "k_proj", "o_proj"],
                                label="Target Modules"
                            )
                            
                            lora_start_btn = gr.Button("üöÄ Start LoRA Training", variant="primary")
                            lora_stop_btn = gr.Button("‚èπÔ∏è Stop Training", variant="secondary")
                        
                        with gr.Column():
                            gr.Markdown("### Training Status")
                            lora_status = gr.Textbox(label="Status", value="Ready", interactive=False)
                            lora_progress = gr.Slider(0, 100, value=0, label="Progress (%)", interactive=False)
                            lora_memory_usage = gr.Textbox(label="Memory Usage", value="0 GB", interactive=False)
                            lora_loss_plot = gr.Plot(label="Training Loss")
                            lora_logs = gr.Textbox(label="Training Logs", lines=6, interactive=False)
                
                # Mode 4: Voice Training
                with gr.TabItem("4Ô∏è‚É£ Voice Training"):
                    with gr.Row():
                        with gr.Column():
                            gr.Markdown("### Configuration")
                            voice_name = gr.Textbox(label="Voice Name", placeholder="my_voice")
                            voice_speaker = gr.Textbox(label="Speaker ID", placeholder="speaker_001")
                            voice_emotion = gr.Dropdown(
                                choices=["neutral", "happy", "sad", "angry", "surprised", "fearful"],
                                label="Emotion",
                                value="neutral"
                            )
                            
                            voice_audio_files = gr.File(
                                label="Audio Files",
                                file_count="multiple",
                                file_types=["audio"]
                            )
                            voice_output_dir = gr.Textbox(label="Output Directory", value="./custom_voices")
                            
                            with gr.Row():
                                voice_quality = gr.Dropdown(
                                    choices=["standard", "high", "ultra"],
                                    label="Quality",
                                    value="high"
                                )
                                voice_epochs = gr.Slider(10, 200, value=50, label="Epochs")
                            
                            voice_start_btn = gr.Button("üé§ Start Voice Training", variant="primary")
                            voice_stop_btn = gr.Button("‚èπÔ∏è Stop Training", variant="secondary")
                            analyze_voices_btn = gr.Button("üîç Analyze Official Voices", variant="secondary")
                        
                        with gr.Column():
                            gr.Markdown("### Training Status")
                            voice_status = gr.Textbox(label="Status", value="Ready", interactive=False)
                            voice_progress = gr.Slider(0, 100, value=0, label="Progress (%)", interactive=False)
                            voice_audio_preview = gr.Audio(label="Voice Preview", type="numpy")
                            voice_quality_metrics = gr.Textbox(label="Quality Metrics", lines=4, interactive=False)
                            voice_logs = gr.Textbox(label="Training Logs", lines=6, interactive=False)
            
            def start_voice_training(voice_name, voice_speaker, voice_emotion, voice_audio_files, voice_output_dir, voice_quality, voice_epochs):
                """Start voice training process"""

                if not voice_audio_files:
                    return "‚ùå No audio files provided", 0, "No audio files to process", ""
                
                if not voice_name.strip():
                    return "‚ùå Voice name is required", 0, "Please provide a voice name", ""
                
                try:
                    # Create output directory
                    output_path = Path(voice_output_dir) / voice_name
                    output_path.mkdir(parents=True, exist_ok=True)
                    
                    logs = []
                    
                    # Create detailed log file for this training session
                    import datetime
                    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    log_file_path = output_path / f"{voice_name}_training_log_{timestamp}.txt"
                    
                    def log_and_save(message):
                        """Add to logs and immediately save to file"""
                        logs.append(message)
                        with open(log_file_path, 'a', encoding='utf-8') as f:
                            f.write(message + '\n')
                        print(message)  # Also print to console
                    log_and_save(f"üé§ Starting voice training: {voice_name}")
                    log_and_save(f"üìÅ Output directory: {output_path}")
                    log_and_save(f"üé≠ Emotion: {voice_emotion}")
                    log_and_save(f"üîß Quality: {voice_quality}")
                    log_and_save(f"üìä Epochs: {voice_epochs}")
                    log_and_save(f"üìÇ Audio files: {len(voice_audio_files)}")
                    log_and_save("")
                    
                    # Process audio files
                    processed_files = []
                    for i, audio_file in enumerate(voice_audio_files):
                        logs.append(f"Processing file {i+1}/{len(voice_audio_files)}: {audio_file.name}")
                        
                        # Load and validate audio
                        try:
                            audio_data, sample_rate = sphn.read(audio_file.name)
                            duration = len(audio_data) / sample_rate
                            logs.append(f"  ‚úÖ Duration: {duration:.2f}s, Sample rate: {sample_rate}Hz")
                            
                            # Copy to output directory
                            dest_path = output_path / f"{voice_speaker}_{voice_emotion}_{i:03d}.wav"
                            shutil.copy2(audio_file.name, dest_path)
                            processed_files.append(dest_path)
                            
                        except Exception as e:
                            logs.append(f"  ‚ùå Error processing {audio_file.name}: {str(e)}")
                            continue
                    
                    if not processed_files:
                        return "‚ùå No valid audio files processed", 0, "\n".join(logs), ""
                    
                    logs.append(f"\n‚úÖ Processed {len(processed_files)} audio files")
                    logs.append("\nüéØ Starting voice embedding extraction...")
                    
                    # Load TTS model for voice extraction
                    if not HAS_MOSHI:
                        logs.append("‚ùå Moshi not available - cannot extract voice embeddings")
                        return "‚ùå Moshi required for voice training", 0, "\n".join(logs), ""
                    
                    device = "cuda" if torch.cuda.is_available() else "cpu"
                    logs.append(f"üñ•Ô∏è Using device: {device}")
                    
                    try:
                        # Load TTS model
                        tts_model = load_tts_model(DEFAULT_DSM_TTS_REPO if HAS_MOSHI else "kyutai/dsm-tts-1b-en", device)
                        logs.append("‚úÖ TTS model loaded")
                        
                        # Extract voice embeddings
                        voice_embeddings = []
                        for i, audio_path in enumerate(processed_files):
                            progress = int((i / len(processed_files)) * 50)
                            logs.append(f"Extracting embeddings from {audio_path.name}... ({progress}%)")
                            
                            # Load audio
                            audio, sr = sphn.read(str(audio_path))
                            logs.append(f"  Original audio shape: {audio.shape if hasattr(audio,'shape') else 'N/A'}, dtype: {audio.dtype}")
                            
                            # Convert to mono if multi-channel
                            import numpy as np
                            if len(audio.shape) > 1:
                                audio = audio.mean(axis=0)  # Fix: use axis=0 for proper mono conversion
                            
                            audio_tensor = torch.from_numpy(audio.astype(np.float32)).to(device)
                            logs.append(f"  Converted tensor dtype: {audio_tensor.dtype}, shape: {audio_tensor.shape}")
                            
                            # Resample if necessary (placeholder, implement if needed)
                            if sr != tts_model.mimi.sample_rate:
                                import julius  # You need this library
                                audio_tensor = julius.resample_frac(audio_tensor, sr, tts_model.mimi.sample_rate)
                                logs.append(f"  Resampled from {sr}Hz to {tts_model.mimi.sample_rate}Hz")
                            
                            with torch.no_grad():
                                if audio_tensor.dim() == 1:
                                    audio_tensor = audio_tensor.unsqueeze(0).unsqueeze(0)  # (1,1,T)
                                elif audio_tensor.dim() == 2:
                                    audio_tensor = audio_tensor.unsqueeze(0)  # (1,C,T)
                                
                                audio_tensor = audio_tensor.float()
                                
                                frame_size = tts_model.mimi.frame_size
                                if audio_tensor.shape[-1] % frame_size != 0:
                                    to_pad = frame_size - audio_tensor.shape[-1] % frame_size
                                    audio_tensor = torch.nn.functional.pad(audio_tensor, (0, to_pad))
                                
                                logs.append(f"  Final audio shape: {audio_tensor.shape}, dtype: {audio_tensor.dtype}")
                                
                                # Extract embeddings directly from audio tensor
                                try:
                                    # Use TTS model's voice encoder to extract proper embeddings
                                    try:
                                        # Use the TTS model's built-in voice encoding
                                        with torch.no_grad():
                                            # Create a temporary audio file for the TTS model
                                            import tempfile
                                            import numpy as np
                                            
                                            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_wav:
                                                # Save audio as WAV file using best available method
                                                if audio_tensor.dim() == 3 and audio_tensor.shape[0] == 1:
                                                    # Remove batch dimension for saving
                                                    audio_to_save = audio_tensor.squeeze(0)
                                                else:
                                                    audio_to_save = audio_tensor
                                                
                                                # Convert to numpy and ensure proper format
                                                audio_np = audio_to_save.cpu().numpy()
                                                if audio_np.ndim == 2:
                                                    audio_np = audio_np.T  # soundfile expects (frames, channels)
                                                elif audio_np.ndim == 1:
                                                    pass  # mono is fine
                                                
                                                # Try multiple methods to save audio
                                                saved_successfully = False
                                                
                                                if HAS_SOUNDFILE:
                                                    try:
                                                        # Save using soundfile (most reliable)
                                                        sf.write(temp_wav.name, audio_np, tts_model.mimi.sample_rate, format='WAV', subtype='PCM_16')
                                                        saved_successfully = True
                                                        logs.append(f"  ‚úÖ Saved audio using soundfile")
                                                    except Exception as sf_error:
                                                        logs.append(f"  ‚ö†Ô∏è soundfile failed: {sf_error}")
                                                
                                                if not saved_successfully:
                                                    try:
                                                        # Fallback to wave module (built-in Python)
                                                        import wave
                                                        
                                                        # Convert float32 to int16
                                                        if audio_np.dtype == np.float32:
                                                            audio_int16 = (audio_np * 32767).astype(np.int16)
                                                        else:
                                                            audio_int16 = audio_np.astype(np.int16)
                                                        
                                                        with wave.open(temp_wav.name, 'wb') as wav_file:
                                                            wav_file.setnchannels(1 if audio_np.ndim == 1 else audio_np.shape[1])
                                                            wav_file.setsampwidth(2)  # 16-bit
                                                            wav_file.setframerate(tts_model.mimi.sample_rate)
                                                            wav_file.writeframes(audio_int16.tobytes())
                                                        
                                                        saved_successfully = True
                                                        logs.append(f"  ‚úÖ Saved audio using wave module")
                                                    except Exception as wave_error:
                                                        logs.append(f"  ‚ö†Ô∏è wave module failed: {wave_error}")
                                                
                                                if not saved_successfully:
                                                    try:
                                                        # Last resort: try torchaudio if available
                                                        import torchaudio
                                                        torchaudio.save(temp_wav.name, audio_to_save.cpu(), tts_model.mimi.sample_rate)
                                                        saved_successfully = True
                                                        logs.append(f"  ‚úÖ Saved audio using torchaudio")
                                                    except Exception as ta_error:
                                                        logs.append(f"  ‚ùå All audio saving methods failed: {ta_error}")
                                                        raise Exception("Cannot save temporary audio file")
                                                
                                                temp_path = temp_wav.name
                                                
                                                logs.append(f"  üìÅ Saved temp audio: {temp_path}, shape: {audio_to_save.shape}")
                                            
                                            try:
                                                # Use TTS model's voice extraction
                                                logs.append(f"  üîç Extracting voice characteristics from: {temp_path}")
                                                condition_attrs = tts_model.make_condition_attributes([temp_path], cfg_coef=2.0)
                                                
                                                logs.append(f"  üìä Condition attributes type: {type(condition_attrs)}")
                                                if hasattr(condition_attrs, '__dict__'):
                                                    logs.append(f"  üìä Condition attributes keys: {list(condition_attrs.__dict__.keys())}")
                                                
                                                # Try different ways to access the voice embedding
                                                voice_embedding = None
                                                
                                                if hasattr(condition_attrs, 'speaker_wavs'):
                                                    if hasattr(condition_attrs.speaker_wavs, 'tensor'):
                                                        voice_embedding = condition_attrs.speaker_wavs.tensor
                                                        logs.append(f"  ‚úÖ Found via speaker_wavs.tensor: {voice_embedding.shape}")
                                                    elif hasattr(condition_attrs.speaker_wavs, 'data'):
                                                        voice_embedding = condition_attrs.speaker_wavs.data
                                                        logs.append(f"  ‚úÖ Found via speaker_wavs.data: {voice_embedding.shape}")
                                                    else:
                                                        voice_embedding = condition_attrs.speaker_wavs
                                                        logs.append(f"  ‚úÖ Found via speaker_wavs direct: {voice_embedding.shape}")
                                                
                                                elif hasattr(condition_attrs, 'conditioners'):
                                                    for key, value in condition_attrs.conditioners.items():
                                                        if 'speaker' in key.lower():
                                                            voice_embedding = value
                                                            logs.append(f"  ‚úÖ Found via conditioners[{key}]: {voice_embedding.shape}")
                                                            break
                                                
                                                if voice_embedding is not None:
                                                    # Validate embedding
                                                    if torch.all(voice_embedding == 0):
                                                        logs.append(f"  ‚ö†Ô∏è WARNING: Extracted embedding is all zeros!")
                                                    else:
                                                        logs.append(f"  ‚úÖ Valid embedding extracted: min={voice_embedding.min():.6f}, max={voice_embedding.max():.6f}")
                                                    
                                                    voice_embeddings.append(voice_embedding.cpu())
                                                else:
                                                    logs.append("  ‚ùå Failed to find voice embedding in condition attributes")
                                                    logs.append(f"  üîç Available attributes: {dir(condition_attrs)}")
                                                    continue
                                                    
                                            finally:
                                                # Clean up temp file
                                                import os
                                                try:
                                                    os.unlink(temp_path)
                                                except:
                                                    pass
                                    
                                    except Exception as embed_error:
                                        log_and_save(f"  ‚ùå TTS model embedding failed: {embed_error}")
                                        log_and_save("  üö® –ö–†–ò–¢–ò–ß–ù–û: Fallback –º–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏!")
                                        log_and_save("  üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –≥–æ–ª–æ—Å–∞ –∏–ª–∏ —É–ª—É—á—à–∏—Ç–µ –∞—É–¥–∏–æ –∫–∞—á–µ—Å—Ç–≤–æ")
                                        # Improved fallback method for better quality embeddings
                                        log_and_save("  üîÑ Using IMPROVED fallback embedding method...")
                                        
                                        # Ensure audio tensor is properly shaped and normalized
                                        if audio_tensor.dim() == 3:
                                            audio_for_embedding = audio_tensor.squeeze(0)  # Remove batch dimension
                                        else:
                                            audio_for_embedding = audio_tensor
                                        
                                        # Extract comprehensive audio features for better embeddings
                                        try:
                                            # Import numpy at the beginning
                                            import numpy as np
                                            
                                            # Convert to numpy for feature extraction
                                            audio_np = audio_for_embedding.cpu().numpy()
                                            if audio_np.ndim > 1:
                                                audio_np = audio_np.mean(axis=0)  # Convert to mono
                                            
                                            # Extract multiple types of features
                                            features_list = []
                                            
                                            # 1. Spectral features
                                            
                                            spectral_centroid = []
                                            spectral_rolloff = []
                                            
                                            if HAS_SCIPY:
                                                try:
                                                    # Compute spectrogram using scipy
                                                    f, t, Sxx = signal.spectrogram(audio_np, fs=tts_model.mimi.sample_rate, nperseg=512)
                                                    spectral_centroid = np.sum(f[:, np.newaxis] * Sxx, axis=0) / np.sum(Sxx, axis=0)
                                                    for i in range(Sxx.shape[1]):
                                                        cumsum = np.cumsum(Sxx[:, i])
                                                        rolloff_idx = np.where(cumsum >= 0.85 * cumsum[-1])[0]
                                                        if len(rolloff_idx) > 0:
                                                            spectral_rolloff.append(f[rolloff_idx[0]])
                                                        else:
                                                            spectral_rolloff.append(f[-1])
                                                except Exception as scipy_error:
                                                    log_and_save(f"  ‚ö†Ô∏è scipy spectrogram failed: {scipy_error}")
                                            
                                            if not spectral_centroid:
                                                # Basic spectral features without scipy
                                                fft = np.fft.fft(audio_np)
                                                magnitude = np.abs(fft)
                                                freqs = np.fft.fftfreq(len(audio_np), 1/tts_model.mimi.sample_rate)
                                                
                                                # Simple spectral centroid
                                                spectral_centroid = [np.sum(freqs[:len(freqs)//2] * magnitude[:len(freqs)//2]) / np.sum(magnitude[:len(freqs)//2])]
                                                
                                                # Simple rolloff
                                                cumsum = np.cumsum(magnitude[:len(freqs)//2])
                                                rolloff_idx = np.where(cumsum >= 0.85 * cumsum[-1])[0]
                                                if len(rolloff_idx) > 0:
                                                    spectral_rolloff = [freqs[rolloff_idx[0]]]
                                                else:
                                                    spectral_rolloff = [freqs[len(freqs)//4]]
                                            
                                            # 2. Temporal features with voice characteristics preservation
                                            chunk_size = 1024
                                            temporal_features = []
                                            
                                            # Analyze overall audio characteristics for gender preservation
                                            overall_pitch = np.mean(np.abs(np.diff(audio_np)))  # Rough pitch estimate
                                            overall_energy = np.mean(audio_np ** 2)
                                            
                                            for i in range(0, len(audio_np), chunk_size):
                                                chunk = audio_np[i:i+chunk_size]
                                                if len(chunk) > 0:
                                                    # Statistical features
                                                    chunk_mean = np.mean(chunk)
                                                    chunk_std = np.std(chunk)
                                                    chunk_skew = np.mean(((chunk - chunk_mean) / (chunk_std + 1e-8)) ** 3)
                                                    chunk_kurt = np.mean(((chunk - chunk_mean) / (chunk_std + 1e-8)) ** 4)
                                                    
                                                    # Energy features
                                                    chunk_energy = np.sum(chunk ** 2)
                                                    chunk_zcr = np.sum(np.diff(np.sign(chunk)) != 0) / len(chunk)
                                                    
                                                    # Voice characteristics features
                                                    chunk_pitch = np.mean(np.abs(np.diff(chunk)))
                                                    pitch_ratio = chunk_pitch / (overall_pitch + 1e-8)  # Relative pitch
                                                    energy_ratio = chunk_energy / (overall_energy + 1e-8)  # Relative energy
                                                    
                                                    temporal_features.extend([
                                                        chunk_mean, chunk_std, chunk_skew, chunk_kurt,
                                                        chunk_energy, chunk_zcr, pitch_ratio, energy_ratio
                                                    ])
                                            
                                            # 3. Frequency domain features
                                            fft = np.fft.fft(audio_np)
                                            magnitude = np.abs(fft)
                                            phase = np.angle(fft)
                                            
                                            # Take representative samples from frequency domain
                                            freq_features = []
                                            for i in range(0, len(magnitude), len(magnitude) // 100):
                                                freq_features.extend([magnitude[i], phase[i]])
                                            
                                            # 4. Combine all features
                                            all_features = []
                                            all_features.extend(spectral_centroid[:min(50, len(spectral_centroid))])
                                            all_features.extend(spectral_rolloff[:min(50, len(spectral_rolloff))])
                                            all_features.extend(temporal_features[:min(200, len(temporal_features))])
                                            all_features.extend(freq_features[:min(200, len(freq_features))])
                                            
                                            # Convert to tensor
                                            features_tensor = torch.tensor(all_features, dtype=torch.float32)
                                            
                                            # Create proper 3D format like official voices: [1, 512, 125]
                                            target_shape = (1, 512, 125)
                                            target_elements = target_shape[0] * target_shape[1] * target_shape[2]  # 64000
                                            
                                            # Create diverse embedding by repeating and modifying features
                                            final_embedding = torch.zeros(target_elements)
                                            
                                            # Fill with features in a pattern that creates diversity
                                            # Use more sophisticated pattern for better voice quality
                                            for i in range(target_elements):
                                                base_idx = i % len(features_tensor)
                                                
                                                # Multi-frequency variation for better voice characteristics
                                                variation1 = 0.08 * np.sin(i * 0.007) * np.cos(i * 0.0013)  # Low freq
                                                variation2 = 0.04 * np.sin(i * 0.023) * np.cos(i * 0.0037)  # Mid freq  
                                                variation3 = 0.02 * np.sin(i * 0.051) * np.cos(i * 0.0071)  # High freq
                                                
                                                # Controlled noise with better distribution
                                                noise = 0.03 * (np.random.normal(0, 1) * 0.5)  # Gaussian noise
                                                
                                                # Combine all variations
                                                total_variation = variation1 + variation2 + variation3 + noise
                                                final_embedding[i] = features_tensor[base_idx] + total_variation
                                            
                                            # Reshape to 3D format
                                            final_embedding = final_embedding.reshape(target_shape)
                                            
                                            # Advanced normalization for better voice quality
                                            # Apply gentle normalization to preserve voice characteristics
                                            final_embedding = torch.tanh(final_embedding * 0.8)  # Softer saturation
                                            
                                            # Add subtle voice-specific adjustments
                                            # Enhance mid-frequency components (important for speech clarity)
                                            mid_freq_boost = torch.sin(torch.arange(final_embedding.shape[1]).float() * 0.1) * 0.05
                                            final_embedding[0, :, :] += mid_freq_boost.unsqueeze(1)
                                            
                                            # Ensure final normalization
                                            final_embedding = torch.clamp(final_embedding, -0.95, 0.95)
                                            
                                            log_and_save(f"  ‚úÖ IMPROVED voice embedding extracted: {final_embedding.shape}")
                                            log_and_save(f"  üìä Stats: min={final_embedding.min():.4f}, max={final_embedding.max():.4f}, std={final_embedding.std():.4f}")
                                            
                                            # Validate embedding quality
                                            unique_values = torch.unique(final_embedding).numel()
                                            total_values = final_embedding.numel()
                                            uniqueness_ratio = unique_values / total_values
                                            
                                            log_and_save(f"  üìà Quality: {uniqueness_ratio*100:.1f}% unique values ({unique_values}/{total_values})")
                                            
                                            if final_embedding.numel() == 0 or torch.any(torch.isnan(final_embedding)) or torch.any(torch.isinf(final_embedding)):
                                                log_and_save("  ‚ùå Invalid voice embedding, skipping.")
                                                continue
                                            
                                            voice_embeddings.append(final_embedding.cpu())
                                            
                                        except Exception as fallback_error:
                                            log_and_save(f"  ‚ùå Improved fallback failed: {fallback_error}")
                                            log_and_save("  üîÑ Using basic fallback...")
                                            
                                            # Basic fallback as last resort
                                            target_shape = (1, 512, 125)
                                            basic_embedding = torch.randn(target_shape) * 0.1
                                            voice_embeddings.append(basic_embedding.cpu())
                                            log_and_save("  ‚ö†Ô∏è Used basic random embedding (very poor quality)")
                                except Exception as e:
                                    logs.append(f"  ‚ùå Error extracting condition attributes: {str(e)}")
                                    import traceback
                                    logs.append(traceback.format_exc())
                                    continue
                        
                        logs.append(f"Total voice embeddings collected: {len(voice_embeddings)}")
                        
                        if not voice_embeddings:
                            logs.append("‚ùå No voice embeddings extracted")
                            return "‚ùå Failed to extract voice embeddings", 0, "\n".join(logs), ""
                        
                        # Average embeddings
                        try:
                            final_embedding = torch.stack(voice_embeddings).mean(dim=0)
                            logs.append(f"‚úÖ Successfully stacked embeddings")
                        except Exception as e:
                            logs.append(f"‚ùå Error stacking embeddings: {e}")
                            return "‚ùå Error combining embeddings", 0, "\n".join(logs), ""
                        
                        logs.append(f"Final embedding shape: {final_embedding.shape}")
                        
                        if final_embedding.numel() == 0:
                            logs.append("‚ùå Final embedding is empty!")
                            return "‚ùå Empty embedding generated", 0, "\n".join(logs), ""
                        
                        if torch.all(final_embedding == 0):
                            logs.append("‚ö†Ô∏è Warning: Final embedding is all zeros!")
                        
                        if torch.any(torch.isnan(final_embedding)):
                            logs.append("‚ùå Final embedding contains NaN values!")
                            return "‚ùå Invalid embedding (NaN)", 0, "\n".join(logs), ""
                        
                        if torch.any(torch.isinf(final_embedding)):
                            logs.append("‚ùå Final embedding contains infinite values!")
                            return "‚ùå Invalid embedding (inf)", 0, "\n".join(logs), ""
                        
                        # Save voice embedding using safetensors
                        voice_file = output_path / f"{voice_name}_{voice_emotion}.safetensors"
                        final_embedding = final_embedding.contiguous()
                        
                        # –ê–Ω–∞–ª–∏–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
                        logs.append("üîç –ê–Ω–∞–ª–∏–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞...")
                        
                        try:
                            # –ù–∞–π—Ç–∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                            expresso_dir = project_dir / "cache" / "voices" / "expresso"
                            official_files = list(expresso_dir.glob("*.safetensors"))
                            
                            if official_files:
                                official_file = official_files[0]
                                logs.append(f"üìÅ –ê–Ω–∞–ª–∏–∑: {official_file.name}")
                                
                                from safetensors.torch import load_file
                                official_data = load_file(str(official_file))
                                
                                logs.append(f"üîë –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏: {list(official_data.keys())}")
                                
                                for key, tensor in official_data.items():
                                    logs.append(f"  {key}: {tensor.shape}, {tensor.dtype}")
                                
                                # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
                                if 'speaker_wavs' in official_data:
                                    official_shape = official_data['speaker_wavs'].shape
                                    official_dtype = official_data['speaker_wavs'].dtype
                                    
                                    logs.append(f"üéØ –¶–µ–ª–µ–≤–∞—è —Ñ–æ—Ä–º–∞: {official_shape}, {official_dtype}")
                                    
                                    # –ü–æ–¥–æ–≥–Ω–∞—Ç—å –Ω–∞—à —ç–º–±–µ–¥–¥–∏–Ω–≥ –ø–æ–¥ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                                    if len(official_shape) == 2:
                                        # 2D —Ñ–æ—Ä–º–∞—Ç - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–∞—à 1D –≤ 2D
                                        target_h, target_w = official_shape
                                        
                                        # –°–æ–∑–¥–∞—Ç—å 2D —ç–º–±–µ–¥–¥–∏–Ω–≥ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                                        if final_embedding.shape[0] < target_h * target_w:
                                            # –î–æ–ø–æ–ª–Ω–∏—Ç—å –Ω—É–ª—è–º–∏
                                            padding_size = target_h * target_w - final_embedding.shape[0]
                                            final_embedding = torch.cat([final_embedding, torch.zeros(padding_size)])
                                        elif final_embedding.shape[0] > target_h * target_w:
                                            # –û–±—Ä–µ–∑–∞—Ç—å
                                            final_embedding = final_embedding[:target_h * target_w]
                                        
                                        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ 2D
                                        final_embedding = final_embedding.reshape(target_h, target_w)
                                        logs.append(f"‚úÖ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ –≤ 2D: {final_embedding.shape}")
                                    
                                    # –ü—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Ç–∏–ø—É –¥–∞–Ω–Ω—ã—Ö
                                    final_embedding = final_embedding.to(official_dtype)
                                    logs.append(f"‚úÖ –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {final_embedding.dtype}")
                                    
                                else:
                                    logs.append("‚ö†Ô∏è 'speaker_wavs' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏")
                            else:
                                logs.append("‚ö†Ô∏è –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                        
                        except Exception as e:
                            logs.append(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
                        
                        voice_data = {'speaker_wavs': final_embedding}
                        
                        logs.append(f"Saving {len(voice_data)} tensors...")
                        try:
                            from safetensors.torch import save_file
                            import json
                            import datetime
                            import traceback
                            
                            save_file(voice_data, str(voice_file))
                            file_size = voice_file.stat().st_size
                            logs.append(f"File saved: {voice_file} ({file_size} bytes)")
                            
                            if file_size == 0:
                                logs.append("‚ùå WARNING: Saved file is 0 bytes!")
                                torch.save(voice_data, str(voice_file).replace('.safetensors', '_backup.pt'))
                                logs.append("Saved backup as .pt file")
                            
                            # Save metadata JSON
                            metadata = {
                                'name': voice_name,
                                'speaker': voice_speaker,
                                'emotion': voice_emotion,
                                'quality': voice_quality,
                                'num_files': len(processed_files),
                                'created': datetime.datetime.now().isoformat(),
                                'embedding_shape': list(final_embedding.shape),
                                'sample_rate': tts_model.mimi.sample_rate,
                                'tensor_info': {
                                    'dtype': str(final_embedding.dtype),
                                    'numel': final_embedding.numel(),
                                    'is_contiguous': final_embedding.is_contiguous()
                                }
                            }
                            
                            metadata_file = output_path / f"{voice_name}_{voice_emotion}_metadata.json"
                            with open(metadata_file, 'w') as f:
                                json.dump(metadata, f, indent=2)
                            logs.append(f"‚úÖ Metadata saved: {metadata_file}")
                            
                            # Generate preview audio
                            preview_text = f"Hello, this is {voice_name} speaking with {voice_emotion} emotion."
                            logs.append("üéµ Generating voice preview...")
                            
                            try:
                                # Fix path issue for preview generation
                                voice_file_relative = voice_file.relative_to(project_dir)
                                voice_file_str = str(voice_file_relative).replace('\\', '/')
                                
                                sample_rate, preview_audio = synthesize_speech(
                                    preview_text,
                                    f"üé§ {voice_name} ({voice_emotion})",  # Use display name format
                                    DEFAULT_DSM_TTS_REPO if HAS_MOSHI else "kyutai/dsm-tts-1b-en",
                                    device
                                )
                                logs.append("‚úÖ Voice preview generated successfully!")
                            except Exception as preview_error:
                                logs.append(f"‚ö†Ô∏è Preview generation failed: {preview_error}")
                                logs.append("Voice training completed, but preview unavailable")
                            
                            logs.append("‚úÖ Voice training completed successfully!")
                            logs.append(f"üìÅ Voice file: {voice_file}")
                            logs.append(f"üìÑ Metadata file: {metadata_file}")
                            logs.append(f"üé§ Voice ready for use in TTS")
                            
                            metrics = f"""Voice Quality Metrics:
            ‚Ä¢ Files processed: {len(processed_files)}
            ‚Ä¢ Total duration: {sum(len(sphn.read(str(f))[0]) / sphn.read(str(f))[1] for f in processed_files):.1f}s
            ‚Ä¢ Embedding size: {final_embedding.shape}
            ‚Ä¢ Device used: {device}
            ‚Ä¢ Quality level: {voice_quality}"""
                            
                            return "‚úÖ Voice training completed!", 100, "\n".join(logs), metrics
                            
                        except Exception as e:
                            logs.append(f"‚ùå Error saving voice: {str(e)}")
                            return "‚ùå Error saving voice", 0, "\n".join(logs), ""
                        
                    except Exception as e:
                        logs.append(f"‚ùå Training error: {str(e)}")
                        return f"‚ùå Training failed: {str(e)}", 0, "\n".join(logs), ""
                    
                except Exception as e:
                    return f"‚ùå Error: {str(e)}", 0, f"Error during voice training: {str(e)}", ""
            
            def start_training(training_type, *args):
                if training_type == "Voice":
                    return start_voice_training(*args)
                else:
                    return f"Starting {training_type} training...", 0
            
            def stop_training(training_type):
                return f"{training_type} training stopped.", 0
            
            # Connect buttons
            ft_start_btn.click(
                fn=lambda *args: start_training("Fine-tuning", *args),
                inputs=[ft_base_model, ft_dataset_path, ft_output_dir, ft_epochs, ft_batch_size, ft_learning_rate, ft_warmup_steps],
                outputs=[ft_status, ft_progress]
            )
            
            lora_start_btn.click(
                fn=lambda *args: start_training("LoRA", *args),
                inputs=[lora_base_model, lora_dataset_path, lora_output_dir, lora_rank, lora_alpha, lora_dropout, lora_epochs, lora_target_modules],
                outputs=[lora_status, lora_progress]
            )
            
            voice_start_btn.click(
                fn=lambda *args: start_training("Voice", *args),
                inputs=[voice_name, voice_speaker, voice_emotion, voice_audio_files, voice_output_dir, voice_quality, voice_epochs],
                outputs=[voice_status, voice_progress, voice_logs, voice_quality_metrics]
            )
            
            # ----------------------------------------
            gr.Markdown("---")
            
            # INFO Spoiler with RAW data
            with gr.Accordion("‚ÑπÔ∏è INFO - Training & Languages RAW Data", open=False):
                def get_raw_training_info():
                    return 
                raw_info_display = gr.Textbox(
                    label="TBD",
                    value=get_raw_training_info(),
                    lines=25,
                    interactive=False,
                    show_copy_button=True
                )
        

        # System Tab
        with gr.Tab("üîß System"):
            with gr.Row():
                with gr.Column():
                    def get_system_info():
                        info = []
                        info.append(f"Python: {sys.version.split()[0]}")
                        info.append(f"PyTorch: {torch.__version__}")
                        info.append(f"CUDA: {torch.cuda.is_available()}")
                        if torch.cuda.is_available():
                            info.append(f"GPU: {torch.cuda.get_device_name()}")
                        info.append(f"Moshi: {HAS_MOSHI}")
                        info.append(f"MLX: {HAS_MLX}")
                        
                        # Disk space
                        import shutil
                        usage = shutil.disk_usage('.')
                        info.append(f"Free Space: {usage.free / 1024**3:.1f} GB")
                        
                        # Cache info
                        cache_dir = project_dir / "cache"
                        if cache_dir.exists():
                            cache_size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())
                            info.append(f"Cache Size: {cache_size / 1024**3:.2f} GB")
                        
                        return "\n".join(info)
                    
                    system_info = gr.Textbox(label="System Information", value=get_system_info(), lines=15)
                    refresh_btn = gr.Button("üîÑ Refresh")
                    
                    def clear_cache():
                        global model_cache
                        model_cache.clear()
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        return "‚úÖ Cache cleared"
                    
                    clear_btn = gr.Button("üóëÔ∏è Clear Cache")
                    cache_status = gr.Textbox(label="Status")
                    
                    refresh_btn.click(get_system_info, outputs=system_info)
                    clear_btn.click(clear_cache, outputs=cache_status)
        
        gr.Markdown("""
        ---
        **Kyutai STT & TTS** | Portable ‚Ä¢ Complete ‚Ä¢ Simple
        
        üìÅ Cache: `./cache/` | üìÑ Temp: `./temp/` | üîó [Repository](https://github.com/kyutai-labs/delayed-streams-modeling)
        """)
    
    return demo

def main():
    """Main function"""
    # Setup dual logging - both console and file
    log_file_path = project_dir / "webui_log.txt"
    
    class DualLogger:
        def __init__(self, file_path):
            self.terminal = sys.stdout
            self.log_file = open(file_path, "w", encoding="utf-8")
        
        def write(self, message):
            self.terminal.write(message)
            self.log_file.write(message)
            self.terminal.flush()
            self.log_file.flush()
        
        def flush(self):
            self.terminal.flush()
            self.log_file.flush()
        
        def close(self):
            self.log_file.close()
        
        def isatty(self):
            # Required for uvicorn compatibility
            return self.terminal.isatty() if hasattr(self.terminal, 'isatty') else False
        
        def fileno(self):
            # Required for some logging systems
            return self.terminal.fileno() if hasattr(self.terminal, 'fileno') else None
        
        def __getattr__(self, name):
            # Delegate any other attributes to terminal
            return getattr(self.terminal, name)
    
    dual_logger = None
    try:
        dual_logger = DualLogger(log_file_path)
        sys.stdout = dual_logger
        sys.stderr = dual_logger
        print(f"üöÄ Starting Kyutai STT & TTS WebUI. Logs: console + {log_file_path}")
    except Exception as e:
        print(f"‚ùå Failed to set up dual logging: {e}")
        print("Continuing with console logging only.")

    try:
        print("üöÄ Starting Kyutai STT & TTS WebUI...")

        # Check dependencies
        if not HAS_MOSHI:
            print("‚ö†Ô∏è Moshi not available. Some features will be limited.")
            print("Run setup to install all dependencies.")

        # Find port
        port = find_free_port()
        if port is None:
            port = 7860

        print(f"üåê Starting on port {port}")
        print(f"üìÅ Project: {project_dir}")
        print(f"üóÇÔ∏è Cache: {project_dir / 'cache'}")

        # Create and launch interface
        demo = create_interface()

        demo.launch(
            server_name="0.0.0.0",
            server_port=port,
            share=False,
            show_error=True,
            inbrowser=True  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫—Ä—ã—Ç—å –±—Ä–∞—É–∑–µ—Ä
        )

    finally:
        if dual_logger is not None:
            sys.stdout = sys.__stdout__
            sys.stderr = sys.__stderr__
            dual_logger.close()
            print(f"‚úÖ Logs saved to {log_file_path}")

if __name__ == "__main__":
    main()
